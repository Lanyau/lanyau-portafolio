[
  {
    "objectID": "proyects_Blog.html",
    "href": "proyects_Blog.html",
    "title": "Proyectos",
    "section": "",
    "text": "Predicción de serie de tiempo de NDVI con ARIMA\n\n\n\nPython\n\nSeries de tiempo\n\nARIMA\n\nGoogle Earth Engine\n\nGIS\n\n\n\n\n\n\n\n\n\nMay 27, 2025\n\n\nRamon David P. Lanyau\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning para predecir, clientes que potencialmente querrán irse\n\n\n\nPython\n\nMachine Learning\n\nscikit-learn\n\nmarketing\n\nregresión logistica\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nRamon David P. Lanyau\n\n\n\n\n\n\n\n\n\n\n\n\nVisualización de la pluma del río Amazonas, con datos NetCDF\n\n\n\nGIS\n\nR\n\nNetCDF\n\nOceanología\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nRamon David P. Lanyau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first-python/python_ml_1.html",
    "href": "posts/first-python/python_ml_1.html",
    "title": "Machine learning para predecir, clientes que potencialmente querrán irse",
    "section": "",
    "text": "Regresión Logística: Prediciendo la Probabilidad de Abandono\n\n\n\nFigura 1: Regresión logística\n\n\nLa figura 1 ilustra el concepto fundamental detrás del algoritmo de Regresión Logística. Aunque los datos en la imagen son bidimensionales, el principio se extiende a múltiples variables predictoras.\nEn esencia, la Regresión Logística es un algoritmo de clasificación binaria. Esto significa que su objetivo es predecir la probabilidad de que una observación pertenezca a una de dos categorías posibles (en la imagen, representadas como ‘0’s’ en verde y ‘1’s’ en naranja).\n¿Cómo funciona?\nA diferencia de la regresión lineal, que predice un valor continuo, la Regresión Logística utiliza una función sigmoide (la curva en forma de “S” roja en la imagen) para transformar la salida de una ecuación lineal en un valor de probabilidad que se encuentra entre 0 y 1.\nInterpretación de la Salida:\n\nValores cercanos a 1: Indican una alta probabilidad de que el usuario pertenezca a la categoría ‘1’ (en tu caso, que abandonará la suscripción).\nValores cercanos a 0: Indican una baja probabilidad de que el usuario pertenezca a la categoría ‘1’, lo que implica una alta probabilidad de pertenecer a la categoría ‘0’ (en tu caso, que no abandonará la suscripción).\n\nAplicación al Abandono de Suscripciones:\n\nDatos de Entrada: Se recopilan datos históricos de los usuarios, incluyendo las variables predictoras mencionadas (nivel de educación, dirección, salario, edad, etc.) y la variable objetivo: si el usuario finalmente abandonó la suscripción (codificado como 1) o no (codificado como 0).\nEntrenamiento del Modelo: Se entrena un modelo de Regresión Logística utilizando estos datos históricos. El algoritmo ajustará los coeficientes de las variables predictoras para encontrar la mejor manera de separar a los usuarios que abandonaron de los que no, basándose en sus características.\nPredicción: Una vez entrenado el modelo, se pueden ingresar los datos de un nuevo usuario (nivel de educación, dirección, salario, edad, etc.) y el modelo predecirá la probabilidad de que ese usuario abandone la suscripción.\nIdentificación de Usuarios en Riesgo: Al aplicar un umbral (por ejemplo, 0.5), la empresa puede identificar a los usuarios con una alta probabilidad de abandono. Estos usuarios son los que “tenderán a abandonar el servicio” y, por lo tanto, se beneficiarían de una “mejor atención al cliente para mantenerlos”.\n\n\n\nInstalando e importando las librerías necesarias en el ambiente de Python\n\nlibrary(reticulate)\n\npy_install(\n  c(\"numpy\", \"pandas\", \"scipy\", \"scikit-learn\", \"matplotlib\"),\n  pip = TRUE\n)\n\n\n\nSe importan las librerías\n\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n\n\nSe descarga la base de datos csv con su url\n\nchurn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\nchurn_df.head()\n\n   tenure   age  address  income   ed  ...  loglong  logtoll  lninc  custcat  churn\n0    11.0  33.0      7.0   136.0  5.0  ...    1.482    3.033  4.913      4.0    1.0\n1    33.0  33.0     12.0    33.0  2.0  ...    2.246    3.240  3.497      1.0    1.0\n2    23.0  30.0      9.0    30.0  1.0  ...    1.841    3.240  3.401      3.0    0.0\n3    38.0  35.0      5.0    76.0  2.0  ...    1.800    3.807  4.331      4.0    0.0\n4     7.0  35.0     14.0    80.0  2.0  ...    1.960    3.091  4.382      3.0    0.0\n\n[5 rows x 28 columns]\n\n\n\n\nSelección de las características necesarias para crear nuestro modelo\nEn este bloque de código vamos a seleccionar las columnas de nuestra base de datos que serán empleadas para predecir qué clientes abandonarán el servicio al cual están inscritos, según sus datos. La columna “churn”, que expresa con 1 si ese cliente abandonó el servicio y con 0 si permaneció, será transformada a tipo entero (int), dado que inicialmente se encuentra como flotante.\n\nchurn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\nchurn_df['churn'] = churn_df['churn'].astype('int')\nchurn_df.head()\n\n   tenure   age  address  income   ed  employ  equip  callcard  wireless  churn\n0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0      1\n1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0      1\n2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0      0\n3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0      0\n4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0      0\n\n\nVemos que las columnas (‘características’) añadidas a nuestra nueva tabla era las que deseabamos para entrenar nuestro modelo de regresión logística\n\nchurn_df.columns\n\nIndex(['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',\n       'callcard', 'wireless', 'churn'],\n      dtype='object')\n\n\nPara crear nuestro modelo necesitamos definir cuales son las variables de entrada, y las añadimos a una variable X.\n\nX = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\nX[0:5]\n\narray([[ 11.,  33.,   7., 136.,   5.,   5.,   0.],\n       [ 33.,  33.,  12.,  33.,   2.,   0.,   0.],\n       [ 23.,  30.,   9.,  30.,   1.,   2.,   0.],\n       [ 38.,  35.,   5.,  76.,   2.,  10.,   1.],\n       [  7.,  35.,  14.,  80.,   2.,  15.,   0.]])\n\n\nTambién nuestro modelo necesitará de una variable de salida que es la que se usa para entrenar al modelo, y es la que posteriormente vamos a predecir (churn) cuando nuestro modelo ya esté entrenado.\n\ny = np.asarray(churn_df['churn'])\ny [0:5]\n\narray([1, 1, 0, 0, 0])\n\n\nAntes de entrenar el modelo debemos normalizar todas las variables de entrada del arreglo X creado, para que todas las variables se encuentren dentro de un rango similar y tengan la misma unidad de medida (Z). Esta normalización se logra restando cada dato menos la media de la columna a la que pertenece ese dato, y dividiéndolo entre la desviación estandar.\n\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\narray([[-1.13518441, -0.62595491, -0.4588971 ,  0.4751423 ,  1.6961288 ,\n        -0.58477841, -0.85972695],\n       [-0.11604313, -0.62595491,  0.03454064, -0.32886061, -0.6433592 ,\n        -1.14437497, -0.85972695],\n       [-0.57928917, -0.85594447, -0.261522  , -0.35227817, -1.42318853,\n        -0.92053635, -0.85972695],\n       [ 0.11557989, -0.47262854, -0.65627219,  0.00679109, -0.6433592 ,\n        -0.02518185,  1.16316   ],\n       [-1.32048283, -0.47262854,  0.23191574,  0.03801451, -0.6433592 ,\n         0.53441472, -0.85972695]])\n\n\n\n\nDivisión de base de datos : Entrenamiento/Prueba\nDividimos nuestra base de datos en un subgrupo de entrenamiento (x de entrenamiento, y de entrenamiento) y otro subgrupo que se usará para probar que tan eficaz es el modelo (x de prueba, y de prueba).\nEn este caso se usará el 20 % de las filas de la base de datos para probar el modelo.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\n\nTrain set: (160, 7) (160,)\n\nprint ('Test set:', X_test.shape,  y_test.shape)\n\nTest set: (40, 7) (40,)\n\n\nVamos a construir nuestro modelo utilizando LogisticRegression del paquete Scikit-learn. Esta función implementa regresión logística y puede utilizar diferentes optimizadores numéricos para encontrar los parámetros, incluyendo los solucionadores ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’ y ‘saga’. Puedes encontrar información detallada sobre las ventajas y desventajas de estos optimizadores si los buscas en internet.\nLa versión de Regresión Logística en Scikit-learn admite regularización, una técnica utilizada para resolver el problema de sobreajuste (overfitting) en modelos de aprendizaje automático.\nEl parámetro C indica el inverso de la fuerza de regularización y debe ser un número flotante positivo. Valores más pequeños indican una regularización más fuerte.\nAhora ajustemos nuestro modelo con el conjunto de entrenamiento:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR\n\nLogisticRegression(C=0.01, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(C=0.01, solver='liblinear') \n\n\nAhora predecimos Y (churn: o tasa de abandono) con las variables de entrada asignadas al arreglo X en el subgrupo de prueba.\n\nyhat = LR.predict(X_test)\nyhat\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n\n\npredict_proba devuelve las estimaciones de probabilidad para todas las clases, ordenadas según las etiquetas de las clases.\n\nLa primera columna corresponde a la probabilidad de la clase 0, es decir, P(Y=0∣X)P(Y=0∣X).\nLa segunda columna corresponde a la probabilidad de la clase 1, es decir, P(Y=1∣X)P(Y=1∣X).\n\n\nyhat_prob = LR.predict_proba(X_test)\nyhat_prob\n\narray([[0.54132919, 0.45867081],\n       [0.60593357, 0.39406643],\n       [0.56277713, 0.43722287],\n       [0.63432489, 0.36567511],\n       [0.56431839, 0.43568161],\n       [0.55386646, 0.44613354],\n       [0.52237207, 0.47762793],\n       [0.60514349, 0.39485651],\n       [0.41069572, 0.58930428],\n       [0.6333873 , 0.3666127 ],\n       [0.58068791, 0.41931209],\n       [0.62768628, 0.37231372],\n       [0.47559883, 0.52440117],\n       [0.4267593 , 0.5732407 ],\n       [0.66172417, 0.33827583],\n       [0.55092315, 0.44907685],\n       [0.51749946, 0.48250054],\n       [0.485743  , 0.514257  ],\n       [0.49011451, 0.50988549],\n       [0.52423349, 0.47576651],\n       [0.61619519, 0.38380481],\n       [0.52696302, 0.47303698],\n       [0.63957168, 0.36042832],\n       [0.52205164, 0.47794836],\n       [0.50572852, 0.49427148],\n       [0.70706202, 0.29293798],\n       [0.55266286, 0.44733714],\n       [0.52271594, 0.47728406],\n       [0.51638863, 0.48361137],\n       [0.71331391, 0.28668609],\n       [0.67862111, 0.32137889],\n       [0.50896403, 0.49103597],\n       [0.42348082, 0.57651918],\n       [0.71495838, 0.28504162],\n       [0.59711064, 0.40288936],\n       [0.63808839, 0.36191161],\n       [0.39957895, 0.60042105],\n       [0.52127638, 0.47872362],\n       [0.65975464, 0.34024536],\n       [0.5114172 , 0.4885828 ]])\n\n\n\n\nEvaluación\nÍndice de jaccard\nProbemos el índice de Jaccard para evaluar la precisión. Podemos definir el índice de Jaccard como el tamaño de la intersección dividido por el tamaño de la unión de los dos conjuntos de etiquetas.\n\nSi el conjunto completo de etiquetas predichas para una muestra coincide exactamente con el conjunto verdadero de etiquetas, la precisión del subconjunto será 1.0.\nDe lo contrario, será 0.0.\n\n\nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test, yhat,pos_label=0)\n\nnp.float64(0.7058823529411765)\n\n\nMatriz de confusión\nOtra forma de observar la presición de nuestro modelo de clasificación es crear una matriz de confusión.\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))\n\n[[ 6  9]\n [ 1 24]]\n\n\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')\n\nConfusion matrix, without normalization\n[[ 6  9]\n [ 1 24]]\n\n\n\n\n\n\n\n\n\nAnalicemos la primera fila. La primera fila corresponde a los clientes cuyo valor real de churn en el conjunto de prueba es 1.\nComo puedes calcular, de 40 clientes, el valor de churn es 1 para 15 de ellos.\nDe estos 15 casos:\n\nEl clasificador predijo correctamente 6 como 1.\nY predijo incorrectamente 9 como 0.\n\nEsto significa que:\n\nPara 6 clientes, el valor real de churn era 1 (en el conjunto de prueba) y el clasificador también los predijo correctamente como 1.\nSin embargo, para 9 clientes cuyo valor real era 1, el clasificador los predijo como 0, lo cual no es muy bueno. Podemos considerarlo como el error del modelo en la primera fila.\n\n¿Qué ocurre con los clientes con churn igual a 0? Veamos la segunda fila:\n\nHubo 25 clientes cuyo valor de churn era 0.\nEl clasificador predijo correctamente 24 de ellos como 0.\nY predijo incorrectamente 1 de ellos como 1.\n\nPor lo tanto, el modelo hizo un buen trabajo prediciendo a los clientes con churn igual a 0.\nVentaja de la matriz de confusión:\nMuestra la capacidad del modelo para predecir correctamente o separar las clases. En el caso específico de un clasificador binario (como este ejemplo), podemos interpretar estos números como:\n\nVerdaderos positivos (VP): Casos correctamente predichos como 1.\nFalsos positivos (FP): Casos predichos como 1 pero que en realidad son 0.\nVerdaderos negativos (VN): Casos correctamente predichos como 0.\nFalsos negativos (FN): Casos predichos como 0 pero que en realidad son 1.\n\n\nprint (classification_report(y_test, yhat))\n\n              precision    recall  f1-score   support\n\n           0       0.73      0.96      0.83        25\n           1       0.86      0.40      0.55        15\n\n    accuracy                           0.75        40\n   macro avg       0.79      0.68      0.69        40\nweighted avg       0.78      0.75      0.72        40\n\n\nLos resultados muestran que el modelo tiene un desempeño diferenciado entre las clases. Para la clase 0 (no churn), logra una precisión del 73%, identificando correctamente el 96% de los casos reales (recall), lo que se refleja en un sólido F1-score de 0.83. Esto indica que el modelo es particularmente bueno detectando clientes que permanecerán. Sin embargo, para la clase 1 (churn), aunque tiene una alta precisión (86%), solo captura el 40% de los casos reales, resultando en un F1-score más bajo (0.55). Esto sugiere dificultades para identificar completamente a los clientes que abandonarán. La exactitud global del modelo es del 75%, pero al analizar el promedio macro de los F1-scores (0.69) y el promedio ponderado (0.72), vemos que el rendimiento es desigual entre clases. El alto recall en clase 0 contrasta con el bajo recall en clase 1, indicando que el modelo tiende a ser conservador al predecir churn, posiblemente para minimizar falsos positivos a costa de perder algunos casos reales.\nlog loss\nAhora, probemos la pérdida logarítmica (log loss) para la evaluación. En la regresión logística, la salida puede ser la probabilidad de que el abandono del cliente sea afirmativo (o igual a 1). Esta probabilidad es un valor entre 0 y 1. La pérdida logarítmica mide el rendimiento de un clasificador donde la salida predicha es un valor de probabilidad entre 0 y 1.\n\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)\n\n0.6017092478101185\n\n\n\nLR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)\nyhat_prob2 = LR2.predict_proba(X_test)\nprint (\"LogLoss: : %.2f\" % log_loss(y_test, yhat_prob2))\n\nLogLoss: : 0.61"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sobre mi",
    "section": "",
    "text": "Master en Manejo en Ciencias de Recursos Marinos\nCientífico de datos con sólido dominio en procesamiento, visualización y SIG. En constante formación, aplico técnicas estadísticas y de machine learning, incluyendo enfoques geoespaciales, para transformar datos en decisiones estratégicas."
  },
  {
    "objectID": "index.html#habilidades",
    "href": "index.html#habilidades",
    "title": "Sobre mi",
    "section": "Habilidades",
    "text": "Habilidades\n\n\n\nHabilidades, programas, lenguajes de programación, módulos\n\n\n\nPython (pandas, numpy, scikit-learn), SQL ,R, QGIS, Google Earth Engine, Git y GitHub"
  },
  {
    "objectID": "index.html#certificaciones.",
    "href": "index.html#certificaciones.",
    "title": "Sobre mi",
    "section": "Certificaciones.",
    "text": "Certificaciones.\n\nPython\n\n“Python for Benginners” by Sololearn: [ver certificado]\n“Intermediate Python” by Sololearn: [ver certificado]\n“Python” de Kaggle: [ver certificado]\n“Pandas” de Kaggle: [ver certificado]\n“Machine Learning with Python - Level 1 by IBM”: [ver certificado]\n\n\n\nR\n\n“Theorical and Practical Understanding of R languaje” by Sololearn: [ver certificado]\n\n\n\nSQL\n\n“Intro to SQL”, de Kaggle: [ver certificado]\n“Introducción a SQL”, de Sololean: [ver certificado]\n“SQL intermedio”, de Sololearn: [ver certificado]\n\n\n\nEstadística, Ciencia de datos, Analisis de datos\n\n“Time Series Workshop” by CICIMAR-IPN: [ver certificado]\n“Data Science: Python for Data Analysis Full Bootcamp” by UDEMY: [ver certificado]\n“Data Analysis with R Programming and Python”: [ver certificado]\n“Free Tensor-Flow Keras Bootcamp” de OpenCV: [ver certificado]\n\n\n\nArcGIS ,QGIS, SNAP\n\n“Spatial Data Science: The New Frontier in Analytics” de ESRI: [ver certificado]\n“Optical Remote Sensing - Introductory Level” de “National Commission on Space Activities (CONAE)”: [ver certificado]\n“Teledetección aplicada al Color del Océano - Nivel Introductorio” de CONAE: [ver certificado]\n“Methane Observations for Large Emission Event Detection and Monitoring”: [ver certificado]\n\n\n\nGoogle Earth Engine, Python\n\n“The Complete Google Earth Engine Python API & Colab Bootcamp” de UDEMY: [ver certificado]\n\n\n\nGit, GitHub\n\n“Basics of Git, GITHUB” de Desafío Latam: [ver certificado]\n\n\n\nMatLab\n\n“MatLab Onramp” de MathWorks: [ver certificado]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Logros",
    "section": "",
    "text": "Denis Ávila, D., Curbelo, E. A., Madrigal-Roca, L. J., & Pérez-Lanyau, R. D. (2020). Spatio temporal variations of the spectral response in mangroves of Havana, Cuba, by remote sensing. Revista De Biología Tropical, 68(1), 321–335. https://doi.org/10.15517/rbt.v68i1.39134\nDenis Ávila, D., Ramírez-Arrieta, V. M., & Pérez-Lanyau, R. D. (2020). Variación espacial de la morfometría foliar en manglares de La Habana, Cuba. Rev. Biol. Trop., 68, 13. http://dx.doi.org/10.15517/rbt.v68i2.39133\nPérez-Leira, R., Pérez-Ojeda, O. L., Pérez-Ojeda, O. M., & Pérez-Lanyau, D. (2025). Estudio exploratorio sobre el régimen de riego en una parcela de pitahaya (Hylocereus undatus). Ingeniería Agrícola, 15, https://cu-id.com/2284/v15e04. Recuperado a partir de https://revistas.unah.edu.cu/index.php/IAgric/article/view/2024"
  },
  {
    "objectID": "about.html#publicaciones-científicas",
    "href": "about.html#publicaciones-científicas",
    "title": "Logros",
    "section": "",
    "text": "Denis Ávila, D., Curbelo, E. A., Madrigal-Roca, L. J., & Pérez-Lanyau, R. D. (2020). Spatio temporal variations of the spectral response in mangroves of Havana, Cuba, by remote sensing. Revista De Biología Tropical, 68(1), 321–335. https://doi.org/10.15517/rbt.v68i1.39134\nDenis Ávila, D., Ramírez-Arrieta, V. M., & Pérez-Lanyau, R. D. (2020). Variación espacial de la morfometría foliar en manglares de La Habana, Cuba. Rev. Biol. Trop., 68, 13. http://dx.doi.org/10.15517/rbt.v68i2.39133\nPérez-Leira, R., Pérez-Ojeda, O. L., Pérez-Ojeda, O. M., & Pérez-Lanyau, D. (2025). Estudio exploratorio sobre el régimen de riego en una parcela de pitahaya (Hylocereus undatus). Ingeniería Agrícola, 15, https://cu-id.com/2284/v15e04. Recuperado a partir de https://revistas.unah.edu.cu/index.php/IAgric/article/view/2024"
  },
  {
    "objectID": "about.html#premios",
    "href": "about.html#premios",
    "title": "Logros",
    "section": "Premios",
    "text": "Premios\n\nPrimer Premio, VI Congreso Estudiantil de Investigación Científica y Tecnología (CEICYT) celebrado en la Universidad del Caribe (UNICARIBE) en Santo Domingo, República Dominicana (2021) en el área temática ‘Internacional’, por el trabajo titulado: ‘Evaluación del efecto Isla de calor urbano en ciudades cubanas y sus variaciones históricas.’\nCONVOCATORIA DE SOLICITUDES: Jóvenes Investigadores Estudiantiles 2024 por el Consejo Sudcaliforniano de Ciencia y Tecnología."
  },
  {
    "objectID": "posts/arima_python/arima_python.html",
    "href": "posts/arima_python/arima_python.html",
    "title": "Predicción de serie de tiempo de NDVI con ARIMA",
    "section": "",
    "text": "En este material se explica cómo acceder a las imágenes satelitales alojadas en Google Earth Engine, una plataforma poderosa que permite el análisis geoespacial a gran escala y el procesamiento eficiente de grandes volúmenes de datos de observación de la Tierra. Se calcula el Índice Vegetal de Diferencia Normalizada (NDVI), un indicador crucial para estimar la salud vegetal en un punto específico durante el período 2013-2022. Luego realizamos análisis de detección de anomalías de esa serie de tiempo, lo cual es importante para identificar eventos inusuales o cambios significativos en la salud de la vegetación que podrían indicar estrés, enfermedad o perturbaciones. Finalmente, emplearemos un modelo ARIMA para predecir cómo será la serie de tiempo de NDVI en ese punto para los siguientes 12 meses, lo que resulta valioso para la planificación agrícola, la gestión de recursos naturales y la alerta temprana de posibles cambios en la vegetación.\nEl modelo ARIMA (Autoregressive Integrated Moving Average) es un modelo estadístico utilizado para analizar y predecir series de tiempo. Se basa en la idea de que los valores futuros de una variable pueden explicarse por sus valores pasados y por los errores de predicción anteriores. El modelo ARIMA se define por tres componentes principales:\n\nAR (Autoregresivo): Utiliza una combinación lineal de los valores pasados de la serie para predecir el valor actual. El orden de AR (p) indica cuántos valores pasados se incluyen en el modelo.\nI (Integrado): Se aplica una diferenciación a la serie de tiempo para hacerla estacionaria (es decir, con media y varianza constantes a lo largo del tiempo). El orden de integración (d) indica cuántas veces se diferencia la serie.\nMA (Promedio Móvil): Utiliza una combinación lineal de los errores de predicción pasados para predecir el valor actual. El orden de MA (q) indica cuántos errores pasados se incluyen en el modelo.\n\nEn resumen, un modelo ARIMA(p, d, q) busca patrones en los datos históricos de una serie de tiempo para realizar pronósticos futuros, teniendo en cuenta tanto la dependencia de los valores pasados como los errores de predicción anteriores.\n\nImportación de los módulos de python y autenticación en Google Earth Engine\n\nimport ee\nimport geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Trigger the authentication flow.\nee.Authenticate()\n\nTrue\n\n# Initialize the library.\nee.Initialize(project='hazel-env-396707')\n\nEn este código mostraremos la media de la coleccion de imágenes Landsat (periodo 2013-2022) ‘LANDSAT/LC08/C02/T1_L2’, y la visualizaremos en color verdadero. Esta colección es la que será empleada para calcular el índice de vegetación normalizado (NDVI).\n\nimagen=ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filterDate('2013-01-01', '2022-12-31').median()\n \n# Obtiene los nombres de las bandas como una lista de ee.String\nband_names_ee_list = imagen.bandNames()\n\n# Para mostrar los nombres de las bandas en la consola (esto se ejecutará en Python)\nband_names = band_names_ee_list.getInfo()\nprint(band_names)\n\n['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'SR_QA_AEROSOL', 'ST_B10', 'ST_ATRAN', 'ST_CDIST', 'ST_DRAD', 'ST_EMIS', 'ST_EMSD', 'ST_QA', 'ST_TRAD', 'ST_URAD', 'QA_PIXEL', 'QA_RADSAT']\n\n# Define the visualization parameters.\nimage_viz_params = {\n    'bands': ['SR_B5', 'SR_B4', 'SR_B3'],\n    'min': 1,\n    'max': 25455    ,\n}\n# Define a map centered on San Francisco Bay.\nmap_l8 = geemap.Map(center=[37.385348,-122.084051], zoom=5)\n# Add the image layer to the map and display it.\nmap_l8.add_layer(imagen,image_viz_params, 'falso color')\nmap_l8\n\n\n\n\n    \n    My Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelimitación de rango de tiempo y localización en la que se extrae la serie de tiempo.\n\n# Define the satellite imagery dataset (e.g., Landsat 8)\ndataset = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n\n# Define the location of interest (e.g., a point or a polygon)\nlocation = ee.Geometry.Point([-122.084051, 37.385348])  # Mountain View, CA\n# Filter the dataset by location and date range\nfiltered_dataset = dataset.filterBounds(location).filterDate('2013-01-01', '2022-12-31')\n\n\n\nCreación de la función de cálculo de NDVI\nEsta función de NDVI creada será iterada con la función map en cada imagen de la colección de imágenes Landsat. El NDVI será calculado a partir de la diferencia de la Banda 5 (región del infrarojo cercano) - Banda 4 (Región del rojo)\n\n# Define a function to calculate NDVI\ndef calculate_ndvi(image):\n    ndvi = image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Map the NDVI calculation function over the filtered dataset\nndvi_dataset = filtered_dataset.map(calculate_ndvi)\n# Define the visualization parameters.\nviz = {\n    'bands': ['NDVI'],\n    'min': 0,\n    'max': 0.25,\n}\n\n\n\nExtracción de la serie de tiempo de NDVI\n\n# Get the time series data for the location\ntime_series_data = ndvi_dataset.getRegion(location, 30).getInfo()\n\n# Extract the time property separately and format it\ntime_values = [item[0] for item in time_series_data[1:]]\ntime_values = pd.to_datetime([item.split('_')[-1] for item in time_values], format='%Y%m%d')\n# Create a pandas DataFrame from the time series data (excluding time)\ndf = pd.DataFrame([item[1:] for item in time_series_data[1:]], columns=time_series_data[0][1:])\ndf['time'] = time_values\n\n\n\nVisualización de la serie de tiempo de NDVI\n\n# Plot the NDVI values over time\nplt.figure(figsize=(12, 6))\nplt.plot(df['time'], df['NDVI'])\nplt.xlabel('Time')\nplt.ylabel('NDVI')\nplt.title('NDVI Serie de tiempo')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLimpieza y agregación de datos en la serie de tiempo de NDVI\nEn este bloque de código , establecemos a la columa ‘Date’ como el índice del DataFrame. Se imputan los datos faltantes (Nan) con la media de toda la serie de tiempo. Y por último se agrega la serie de tiempo a media mensual, estableciendo como fecha de referencia la última fecha de cada mes (‘ME’: Month End).\n\n\nimport statsmodels.api as sm\nndvi_series = df[['time', 'NDVI']].rename(columns={'time': 'Date'}).set_index('Date')\nndvi_series = ndvi_series.fillna(ndvi_series.mean())\n# Resample the data to monthly frequency\nndvi_series = ndvi_series.resample('ME').mean()\n\nEn este código se realiza la descomposición de la serie de tiempo en sus componentes, tendencia, estacionalidad y residuo, con el método aditivo. Lo que permitirá una mejor visualización de las características de esta serie de tiempo.\n\n# Seasonal Decompose\nndvi_series_filled = ndvi_series.fillna(ndvi_series['NDVI'].mean())\ndecomposition = sm.tsa.seasonal_decompose(ndvi_series_filled['NDVI'], model='additive')\ndecomposition.plot()\n\n\n\n\n\n\n\n\nEste código toma una serie de tiempo de NDVI, ajusta un modelo ARIMA(5, 1, 0) a estos datos, pronostica los valores de NDVI para los próximos 12 períodos y luego imprime tanto los valores pronosticados como un resumen estadístico del modelo ajustado.\n\n# Apply ARIMA for forecasting\narima_model = sm.tsa.ARIMA(ndvi_series['NDVI'], order=(5,1,0))\narima_result = arima_model.fit()\n\n# Forecast the next 12 months\nforecast = arima_result.forecast(steps=12)\nprint(forecast)\n\n2023-01-31    0.090206\n2023-02-28    0.093894\n2023-03-31    0.088436\n2023-04-30    0.093845\n2023-05-31    0.101646\n2023-06-30    0.091173\n2023-07-31    0.093757\n2023-08-31    0.095121\n2023-09-30    0.092268\n2023-10-31    0.092873\n2023-11-30    0.094984\n2023-12-31    0.093331\nFreq: ME, Name: predicted_mean, dtype: float64\n\n# Print ARIMA model statistics\nprint(arima_result.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                   NDVI   No. Observations:                  117\nModel:                 ARIMA(5, 1, 0)   Log Likelihood                 203.961\nDate:              mar., 27 may. 2025   AIC                           -395.922\nTime:                        16:52:00   BIC                           -379.401\nSample:                    04-30-2013   HQIC                          -389.216\n                         - 12-31-2022                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.4438      0.091     -4.873      0.000      -0.622      -0.265\nar.L2         -0.2303      0.122     -1.891      0.059      -0.469       0.008\nar.L3         -0.0402      0.123     -0.328      0.743      -0.280       0.200\nar.L4         -0.1265      0.114     -1.109      0.268      -0.350       0.097\nar.L5         -0.2009      0.101     -1.983      0.047      -0.400      -0.002\nsigma2         0.0017      0.000      6.767      0.000       0.001       0.002\n===================================================================================\nLjung-Box (L1) (Q):                   0.17   Jarque-Bera (JB):                 0.90\nProb(Q):                              0.68   Prob(JB):                         0.64\nHeteroskedasticity (H):               0.66   Skew:                            -0.20\nProb(H) (two-sided):                  0.21   Kurtosis:                         2.84\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nDetección de Anomalías en Serie de Tiempo NDVI con Isolation Forest. Este código utiliza la técnica de Isolation Forest para identificar anomalías (valores atípicos) en una serie de tiempo de NDVI. Primero, se imputan los valores faltantes (NaN) en la columna ‘NDVI’ de la serie ndvi_series utilizando la media. Luego, se entrena un modelo de Isolation Forest con una tasa de contaminación del 10% para detectar las anomalías. Finalmente, se imprimen los valores considerados como anomalías y se genera un gráfico de la serie de tiempo con las anomalías resaltadas en rojo.\n\n\nVisualización de serie de tiempo con anomalías detectadas\n\nfrom sklearn.ensemble import IsolationForest\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\n# Crear un imputador para reemplazar NaN con la media\nimputer = SimpleImputer(strategy='mean')\n\n# Aplicar la imputación a la columna 'NDVI'\nndvi_values_imputed = imputer.fit_transform(ndvi_series['NDVI'].values.reshape(-1, 1))\n\n# Aplicar Isolation Forest para detección de anomalías\nX = ndvi_values_imputed\nmodel = IsolationForest(contamination=0.1)\nmodel.fit(X)\n\nIsolationForest(contamination=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  IsolationForest?Documentation for IsolationForestiFittedIsolationForest(contamination=0.1) \n\nanomalies = model.predict(X)\n\n# Detectar anomalías (outliers)\noutliers = X[anomalies == -1]\nprint(outliers)\n\n[[ 0.00795261]\n [ 0.03819191]\n [ 0.02326098]\n [ 0.00233433]\n [ 0.04438574]\n [ 0.16662213]\n [ 0.17310808]\n [-0.00244651]\n [ 0.00589421]\n [ 0.16030437]\n [ 0.16349427]\n [ 0.16698799]]\n\n# Plot the data con anomalías resaltadas\nplt.figure(figsize=(10, 6))\nplt.plot(ndvi_series.index, ndvi_values_imputed, label='NDVI real')\nplt.scatter(ndvi_series.index[anomalies == -1], outliers, color='red', label='Anomalías')\nplt.title('Detección de anomalías usando el método de Isolation Forest') \nplt.xlabel('Time')\nplt.ylabel('NDVI')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPredicción de serie de tiempo para próximos 12 meses con modelo ARIMA.\nEl código toma una serie de tiempo de NDVI, previamente procesada para imputar valores faltantes. Utiliza la función auto_arima de la biblioteca pmdarima para encontrar automáticamente el mejor modelo ARIMA que se ajuste a los datos, considerando la posible estacionalidad con un período de 12 (asumiendo datos mensuales). Una vez que el modelo óptimo es identificado y ajustado, se utiliza para predecir los valores de NDVI para los próximos 12 períodos futuros. Finalmente, el código genera un gráfico que muestra la serie de tiempo real junto con los valores predichos para los siguientes 12 meses, facilitando la visualización del pronóstico.\n\nimport pmdarima as pm\nimport pandas as pd  # Asegúrate de que pandas esté importado si aún no lo está\nimport matplotlib.pyplot as plt # Asegúrate de que matplotlib esté importado\n\n# Asumiendo que 'ndvi_values_imputed' fue creado en el chunk anterior\n# y contiene la serie 'NDVI' con los NaN imputados con la media\n\n# Auto ARIMA model\nauto_arima_model = pm.auto_arima(ndvi_values_imputed, seasonal=True, m=12)\n\n# Forecast future values\nforecast = auto_arima_model.predict(n_periods=12)\nforecast_dates = [ndvi_series.index[-1] + pd.DateOffset(months=i) for i in range(1, 13)]\n\n# Plot the forecasted values\nplt.figure(figsize=(10, 6))\nplt.plot(ndvi_series.index, ndvi_values_imputed, label='NDVI real')\nplt.plot(forecast_dates, forecast, label='Predicción', linestyle='--', marker='o')\nplt.legend()\nplt.title('Predicción del NDVI')\nplt.xlabel('Date')\nplt.ylabel('NDVI')\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Visualización de la pluma del río Amazonas, con datos NetCDF",
    "section": "",
    "text": "En esta publicación voy a explicar como hacer una animación de corrientes oceánicas y salinidad con datos en formato NetCDF correspondientes a la desembocadura del río Amazonas con el lenguaje de programación R.\n\nActivación de librerías de R para manejo y visualización de datos geográficos.\n\n#Empieza aquí el verdadero trabajo \n\nlibrary(ncdf4)\nlibrary(raster)\nlibrary(sp)\nlibrary(sf)\nlibrary(terra)\nlibrary(RColorBrewer)\nlibrary(rasterVis)\nlibrary(spDataLarge)\nlibrary(lwgeom)\nlibrary(mapdata)\nlibrary(latticeExtra)\nlibrary(magick)\nlibrary(RColorBrewer)\nlibrary(scales)\nlibrary(lwgeom)\nlibrary(mapdata)\nlibrary(viridis)\n\n\n\nExtracción de las variables del archivo NetCDF\nEl archivo NetCDF tiene variables de temperatura, salinidad, batimetría, componente zonal, componente meridional, altimetría y densidad , entre otras. Y las variables globales latitud, longitud, tiempo y profundidad que son datos estructurados en forma de vector y que sirven para darle estructura e indexar a las anteriores variables mencionadas (temperatura, densidad, salinidad etc…) las cuales están estructuradas en formato de arreglo con 4 dimensiones : longitud, latitud, profundidad y tiempo. Este archivo NetCDF puede ser descargado aquí.\n\n#Se especifica la direccion de la base de datos NetCDF\ndirsemanal = \"A:/CICIMAR/Bases de datos/Output file of weekly climatology simulated with the ROMS model (NetCDF standard).nc\"\n#se le asigna a una variable la lectura de esos datos\nsemanal = nc_open(dirsemanal)\n#Se muestra en pantalla las variables que presenta la base de datos,\n#y sus dimensiones\nprint(semanal) #consultando metadatos del fichero\n\nFile A:/CICIMAR/Bases de datos/Output file of weekly climatology simulated with the ROMS model (NetCDF standard).nc (NC_FORMAT_CLASSIC):\n\n     9 variables (excluding dimension variables):\n        double h[lon,lat]   \n            standard_name: Bathymetry\n            long_name: Bathymetry\n            units: m\n            missing_value: NaN\n            FillValue: NaN\n        double ssh[lon,lat,time]   \n            standard_name: SSH\n            long_name: Sea Surface Height\n            units: m\n            missing_value: NaN\n            FillValue: NaN\n        double temp[lon,lat,depth,time]   \n            standard_name: temp\n            long_name: Potential temperature\n            units: degreeC\n            missing_value: NaN\n            FillValue: NaN\n        double salt[lon,lat,depth,time]   \n            standard_name: salt\n            long_name: Salinity\n            units: PSU\n            missing_value: NaN\n            FillValue: NaN\n        double swd[lon,lat,depth,time]   \n            standard_name: Density\n            long_name: Seawater density\n            units: Kg m^-3\n            missing_value: NaN\n            FillValue: NaN\n        double ucurr[lon,lat,depth,time]   \n            standard_name: ucurr\n            long_name: Zonal component\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double vcurr[lon,lat,depth,time]   \n            standard_name: vcurr\n            long_name: Meridional component\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double vort[lon,lat,depth,time]   \n            standard_name: vort\n            long_name: Vorticity\n            units: s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double cspd[lon,lat,depth,time]   \n            standard_name: cspd\n            long_name: Current speed\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n\n     4 dimensions:\n        lon  Size:183 \n            standard_name: logitude\n            long_name: logitude\n            units: degrees_east\n            axis: X\n        lat  Size:159 \n            standard_name: latitude\n            long_name: latitude\n            units: degrees_north\n            axis: Y\n        depth  Size:61 \n            standard_name: Depth\n            long_name: Vertical axis\n            units: m\n            axis: Z\n        time  Size:48   *** is unlimited *** \n            standard_name: time\n            long_name: time\n            calendar: standard\n            axis: T\n            comment: \n\n    5 global attributes:\n        creation_date: 05-Aug-2022 01:24:56\n        Producer: DOCEAN/UFPE/Recife/PE/Brasil\n        dataType: 2D Grid\n        Comment: ArpHDv2 datasets (standard NetCDF files)\n        Author: H. L. Varona & M. Araujo\n\n#Una vez visto cuales son las cadenas con las que se nombran a las variables globales\n#procedemos a asignarlas a variables para su posterior extracción del archivo NetCDF\nlat_variable = 'lat'\nlon_variable = 'lon'\ntime_variable ='time'\ndeepvariable= 'depth'\n\n# Extracción de las variables globales\nlatssem = ncvar_get(semanal,lat_variable) \nlonssem = ncvar_get(semanal,lon_variable)\ntimessem = ncvar_get(semanal,time_variable)\ndeepseem=ncvar_get(semanal,deepvariable)\n\n\n#En esta publicación solo vamos a usar la salinidad y los componentes zonales y meridionales para visualizar la magnitud y dirección de las corrientes de los datos\nsalisem = ncvar_get(semanal,\"salt\") \nzonal=ncvar_get(semanal,\"ucurr\")\nmeridional=ncvar_get(semanal,\"vcurr\")\n\n\n\n#Definimos un cuadrante de interes: en las longitudes desde -60 hasta los -40.5 , y en las latitudes desde las -3 hasta las 9.5\n\nlon1=-60\nlon2=-40.5\nlat1=-3\nlat2=9.5\n\n\n\n#Debemos saber que índice ocupan nuestras coordenadas de interes en las variables globales, por eso aplicamos wich para obtener dichos índices en las variables globales.\n#Estos índices serán empleados para indexar las variables ambientales (eg. salinidad) en las dimensiones 1 y 2, que corresponden  a las longitudes y latitudes respectivamente. \n\nindlon1=which(lonssem==lon1)\nindlon2=which(lonssem==lon2)\nindlat1=which(latssem==lat1)\nindlat2=which(latssem==lat2)\n\n\n#Acotando el rango de las variables de latitudes y longitudes para que pueda ser empleado correctamente con las funciones de rastervis\n\n\n#De las variables globales longitud y latitud vamos a extraer unas secciones de estas\n#correspondientes a nuestra region de interes\n#Estos vectores ranlon y ranlat serán utilizados posteriormente para georeferenciar los rasters que se elaboran a partir de las matrices o arreglos de la variable salinidad\nranlon=lonssem[indlon1:indlon2]\nranlat=latssem[indlat1:indlat2]\n\n\n\nAcote de área de interes, rasterización y georeferenciación de las capas de salinidad\nUna vez tenemos las 48 capas de tiempo de la variable de salinidad rasterizada y georeferenciada, podemos proceder a visualizar la capa numero 24 de 48. La cual podemos acceder a ella dentro del elemento 24 de la lista vacía creada para almacenar los rasteres georeferenciados de salinidad\n\n#Creamos una secuencia que se usará para asignar rangos a la variable de salinidad.\n#Desde los 24 UPS (Unidades prácticas de salinidad), hasta los 38 UPS, con saltos de 1.5 UPS\natsal&lt;- seq(24,38,1.5)\n#Le agregamos un título a la leyenda de la escala de colores\nColorkeySal &lt;- list(title=list(\"UPS\"))\n\nlevelplot(lista_vacia[[24]],at=atsal,colorkey = ColorkeySal,par.settings = viridisTheme(region = viridis(10)),margin=FALSE)+ \n  #Se añade una capa a la visualización, correspondiente al shp importado\n    layer(sp.polygons(cntry_sp_wgs84, fill = \"#b28158\"))\n\n\n\n\n\n\n\n\nFigura 1: Visualización de la salinidad en el área de interes para la capa 24.\n\n\nAcote de área de interes, rasterización y georeferenciación de las capas de componente zonal y meridional\n\n##########Creando mapa de dirección de corrientes\n\n\n#Realizamos un acote del área interes nuevamente, pero para las variables de componente zonal y componente meridional \nu=zonal[indlon1:indlon2,indlat1:indlat2,1,]\nv=meridional[indlon1:indlon2,indlat1:indlat2,1,]\n\n\n# Creamos nuevamente listas vacías para las variables de componente zonal y meridional\n\n# Creando una lista vacía con 48 elementos\nlistaU &lt;- vector(\"list\", length = 48)\n# Creando una lista vacía con 48 elementos\nlistaV &lt;- vector(\"list\", length = 48)\n\n\nfor(i in 1:48){\n  #Se rota cada elemento zonal y meridional \n  listaU[[i]] &lt;- apply(t(u[,,i]),2,rev)\n  listaV[[i]] &lt;-apply(t(v[,,i]),2,rev)\n\n  # Se convierte a raster cada elemento de la lista\n  listaU[[i]]=raster(listaU[[i]])\n  listaV[[i]]=raster(listaV[[i]])\n  #Se georeferencia cada elemento raster de las listas vacías con las coordenadas correspondientes a la región de interes\n  extent(listaU[[i]]) &lt;- extent(c(lon1,lon2,lat1,lat2))\n  extent(listaV[[i]]) &lt;- extent(c(lon1,lon2,lat1,lat2)) \n  }\n\n\n\nCreación de una animación a partir de las 48 visualizaciones de salinidad y corrientes superficiales\n\n#creamos un vecor con los nombres de los meses de un año, y que cada mes se repita 4 veces consecutivas, para asignar un mes a cada promedio semanal de las 48 capas de tiempo del archivo\nmeses=rep(c(\"enero\",\"febrero\",\"marzo\",\"abril\",\"mayo\",\"junio\",\"julio\",\"agosto\",\"septiembre\",\"octubre\",\"noviembre\",\"diciembre\"),each=4)\n\n# Inicializar una lista para almacenar cada frame del GIF\nframes &lt;- list()\n\nfor (i in 1:48) {\n  #Se introduce en un brick, al iEsimo componente zonal y meridional  que posteriormente la funcion levelplot usara para calcular la magnitud y velocidad de corrientes con ese brick\n  w &lt;- brick(listaU[[i]], listaV[[i]])\n  \n  #Al objeto w lo multiplicamos por 10 para acentuar las diferencias entre los vectores de las corrientes, como en esta gráfica el tamaño del vector no significa una velocidad específca y solo es representativo de su magnitud se acepta en este caso la multiplicación x 10.\n  u &lt;- vectorplot(w*10, isField = \"dXY\", \n                  par.settings = viridisTheme(region = viridis(10)),\n                  scaleSlope = FALSE, \n                  at = atsal, \n                  region = lista_vacia[[i]], \n                  colorkey = ColorkeySal, \n                  margin = FALSE, \n                  narrows = 1000,\n                  col.arrows = \"black\",\n                  main = paste(\"Dirección y magnitud de corrientes (m/s) y grado de salinidad, \\n mes: \",meses[i], \", semana: \", i,\"/48\", sep = \"\"),\n                  scales = list(\n                    y = list(\n                      at = c(2, 5, 8),\n                      labels = c(\"2\", \"5\", \"8\")\n                    )\n                  )) + \n    layer(sp.polygons(cntry_sp_wgs84, fill = \"#b28158\"))\n  \n  # Guardar cada plot como imagen temporal\n  png(filename = paste0(\"frame_\", sprintf(\"%02d\", i), \".png\"), width = 800, height = 600)\n  print(u)\n  dev.off()\n  \n  # Leer la imagen y añadirla a la lista de frames\n  frames[[i]] &lt;- image_read(paste0(\"frame_\", sprintf(\"%02d\", i), \".png\"))\n}\n\n# Combinar todos los frames en un GIF\ngif &lt;- image_animate(image_join(frames), fps = 5)  # 5 frames por segundo\n\n# Guardar el GIF\nimage_write(gif, \"corrientes_salinidad.gif\")\n\n\n# Opcional: Eliminar los archivos PNG temporales\nfile.remove(list.files(pattern = \"frame_.*\\\\.png\"))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE\n\n\n\n\n\nFigura 2: Secuencia de imagenes de corrientes superficiales y salinidad de la pluma del río Amazonas\n\n\n\n\nReferencias\nR Core Team (2024). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org.\nVarona, H. L., & Araujo, M. (2022). Hydro-thermodynamic dataset of the Amazon River Plume and North Brazil Current retroflection. Data in Brief, 40. https://doi.org/10.17882/82958"
  }
]