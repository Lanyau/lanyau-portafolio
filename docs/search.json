[
  {
    "objectID": "proyects_Blog.html",
    "href": "proyects_Blog.html",
    "title": "Proyectos",
    "section": "",
    "text": "Machine learning para predecir, clientes que potencialmente querrán irse\n\n\n\nPython\n\nMachine Learning\n\nscikit-learn\n\nmarketing\n\nregresión logistica\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nRamon David P. Lanyau\n\n\n\n\n\n\n\n\n\n\n\n\nVisualización de la pluma del río Amazonas, con datos NetCDF\n\n\n\nGIS\n\nR\n\nNetCDF\n\nOceanología\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nRamon David P. Lanyau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first-python/python_ml_1.html",
    "href": "posts/first-python/python_ml_1.html",
    "title": "Machine learning para predecir, clientes que potencialmente querrán irse",
    "section": "",
    "text": "Regresión Logística: Prediciendo la Probabilidad de Abandono\n\n\n\nFigura 1: Regresión logística\n\n\nLa figura 1 ilustra el concepto fundamental detrás del algoritmo de Regresión Logística. Aunque los datos en la imagen son bidimensionales, el principio se extiende a múltiples variables predictoras.\nEn esencia, la Regresión Logística es un algoritmo de clasificación binaria. Esto significa que su objetivo es predecir la probabilidad de que una observación pertenezca a una de dos categorías posibles (en la imagen, representadas como ‘0’s’ en verde y ‘1’s’ en naranja).\n¿Cómo funciona?\nA diferencia de la regresión lineal, que predice un valor continuo, la Regresión Logística utiliza una función sigmoide (la curva en forma de “S” roja en la imagen) para transformar la salida de una ecuación lineal en un valor de probabilidad que se encuentra entre 0 y 1.\nInterpretación de la Salida:\n\nValores cercanos a 1: Indican una alta probabilidad de que el usuario pertenezca a la categoría ‘1’ (en tu caso, que abandonará la suscripción).\nValores cercanos a 0: Indican una baja probabilidad de que el usuario pertenezca a la categoría ‘1’, lo que implica una alta probabilidad de pertenecer a la categoría ‘0’ (en tu caso, que no abandonará la suscripción).\n\nAplicación al Abandono de Suscripciones:\n\nDatos de Entrada: Se recopilan datos históricos de los usuarios, incluyendo las variables predictoras mencionadas (nivel de educación, dirección, salario, edad, etc.) y la variable objetivo: si el usuario finalmente abandonó la suscripción (codificado como 1) o no (codificado como 0).\nEntrenamiento del Modelo: Se entrena un modelo de Regresión Logística utilizando estos datos históricos. El algoritmo ajustará los coeficientes de las variables predictoras para encontrar la mejor manera de separar a los usuarios que abandonaron de los que no, basándose en sus características.\nPredicción: Una vez entrenado el modelo, se pueden ingresar los datos de un nuevo usuario (nivel de educación, dirección, salario, edad, etc.) y el modelo predecirá la probabilidad de que ese usuario abandone la suscripción.\nIdentificación de Usuarios en Riesgo: Al aplicar un umbral (por ejemplo, 0.5), la empresa puede identificar a los usuarios con una alta probabilidad de abandono. Estos usuarios son los que “tenderán a abandonar el servicio” y, por lo tanto, se beneficiarían de una “mejor atención al cliente para mantenerlos”.\n\n\n\nInstalando e importando las librerías necesarias en el ambiente de Python\n\nlibrary(reticulate)\n\npy_install(\n  c(\"numpy\", \"pandas\", \"scipy\", \"scikit-learn\", \"matplotlib\"),\n  pip = TRUE\n)\n\n\n\nSe importan las librerías\n\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n\n\nSe descarga la base de datos csv con su url\n\nchurn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\nchurn_df.head()\n\n   tenure   age  address  income   ed  ...  loglong  logtoll  lninc  custcat  churn\n0    11.0  33.0      7.0   136.0  5.0  ...    1.482    3.033  4.913      4.0    1.0\n1    33.0  33.0     12.0    33.0  2.0  ...    2.246    3.240  3.497      1.0    1.0\n2    23.0  30.0      9.0    30.0  1.0  ...    1.841    3.240  3.401      3.0    0.0\n3    38.0  35.0      5.0    76.0  2.0  ...    1.800    3.807  4.331      4.0    0.0\n4     7.0  35.0     14.0    80.0  2.0  ...    1.960    3.091  4.382      3.0    0.0\n\n[5 rows x 28 columns]\n\n\n\n\nSelección de las características necesarias para crear nuestro modelo\nEn este bloque de código vamos a seleccionar las columnas de nuestra base de datos que serán empleadas para predecir qué clientes abandonarán el servicio al cual están inscritos, según sus datos. La columna “churn”, que expresa con 1 si ese cliente abandonó el servicio y con 0 si permaneció, será transformada a tipo entero (int), dado que inicialmente se encuentra como flotante.\n\nchurn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\nchurn_df['churn'] = churn_df['churn'].astype('int')\nchurn_df.head()\n\n   tenure   age  address  income   ed  employ  equip  callcard  wireless  churn\n0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0      1\n1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0      1\n2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0      0\n3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0      0\n4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0      0\n\n\nVemos que las columnas (‘características’) añadidas a nuestra nueva tabla era las que deseabamos para entrenar nuestro modelo de regresión logística\n\nchurn_df.columns\n\nIndex(['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',\n       'callcard', 'wireless', 'churn'],\n      dtype='object')\n\n\nPara crear nuestro modelo necesitamos definir cuales son las variables de entrada, y las añadimos a una variable X.\n\nX = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\nX[0:5]\n\narray([[ 11.,  33.,   7., 136.,   5.,   5.,   0.],\n       [ 33.,  33.,  12.,  33.,   2.,   0.,   0.],\n       [ 23.,  30.,   9.,  30.,   1.,   2.,   0.],\n       [ 38.,  35.,   5.,  76.,   2.,  10.,   1.],\n       [  7.,  35.,  14.,  80.,   2.,  15.,   0.]])\n\n\nTambién nuestro modelo necesitará de una variable de salida que es la que se usa para entrenar al modelo, y es la que posteriormente vamos a predecir (churn) cuando nuestro modelo ya esté entrenado.\n\ny = np.asarray(churn_df['churn'])\ny [0:5]\n\narray([1, 1, 0, 0, 0])\n\n\nAntes de entrenar el modelo debemos normalizar todas las variables de entrada del arreglo X creado, para que todas las variables se encuentren dentro de un rango similar y tengan la misma unidad de medida (Z). Esta normalización se logra restando cada dato menos la media de la columna a la que pertenece ese dato, y dividiéndolo entre la desviación estandar.\n\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\narray([[-1.13518441, -0.62595491, -0.4588971 ,  0.4751423 ,  1.6961288 ,\n        -0.58477841, -0.85972695],\n       [-0.11604313, -0.62595491,  0.03454064, -0.32886061, -0.6433592 ,\n        -1.14437497, -0.85972695],\n       [-0.57928917, -0.85594447, -0.261522  , -0.35227817, -1.42318853,\n        -0.92053635, -0.85972695],\n       [ 0.11557989, -0.47262854, -0.65627219,  0.00679109, -0.6433592 ,\n        -0.02518185,  1.16316   ],\n       [-1.32048283, -0.47262854,  0.23191574,  0.03801451, -0.6433592 ,\n         0.53441472, -0.85972695]])\n\n\n\n\nDivisión de base de datos : Entrenamiento/Prueba\nDividimos nuestra base de datos en un subgrupo de entrenamiento (x de entrenamiento, y de entrenamiento) y otro subgrupo que se usará para probar que tan eficaz es el modelo (x de prueba, y de prueba).\nEn este caso se usará el 20 % de las filas de la base de datos para probar el modelo.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\n\nTrain set: (160, 7) (160,)\n\nprint ('Test set:', X_test.shape,  y_test.shape)\n\nTest set: (40, 7) (40,)\n\n\nVamos a construir nuestro modelo utilizando LogisticRegression del paquete Scikit-learn. Esta función implementa regresión logística y puede utilizar diferentes optimizadores numéricos para encontrar los parámetros, incluyendo los solucionadores ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’ y ‘saga’. Puedes encontrar información detallada sobre las ventajas y desventajas de estos optimizadores si los buscas en internet.\nLa versión de Regresión Logística en Scikit-learn admite regularización, una técnica utilizada para resolver el problema de sobreajuste (overfitting) en modelos de aprendizaje automático.\nEl parámetro C indica el inverso de la fuerza de regularización y debe ser un número flotante positivo. Valores más pequeños indican una regularización más fuerte.\nAhora ajustemos nuestro modelo con el conjunto de entrenamiento:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR\n\nLogisticRegression(C=0.01, solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(C=0.01, solver='liblinear') \n\n\nAhora predecimos Y (churn: o tasa de abandono) con las variables de entrada asignadas al arreglo X en el subgrupo de prueba.\n\nyhat = LR.predict(X_test)\nyhat\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n\n\npredict_proba devuelve las estimaciones de probabilidad para todas las clases, ordenadas según las etiquetas de las clases.\n\nLa primera columna corresponde a la probabilidad de la clase 0, es decir, P(Y=0∣X)P(Y=0∣X).\nLa segunda columna corresponde a la probabilidad de la clase 1, es decir, P(Y=1∣X)P(Y=1∣X).\n\n\nyhat_prob = LR.predict_proba(X_test)\nyhat_prob\n\narray([[0.54132919, 0.45867081],\n       [0.60593357, 0.39406643],\n       [0.56277713, 0.43722287],\n       [0.63432489, 0.36567511],\n       [0.56431839, 0.43568161],\n       [0.55386646, 0.44613354],\n       [0.52237207, 0.47762793],\n       [0.60514349, 0.39485651],\n       [0.41069572, 0.58930428],\n       [0.6333873 , 0.3666127 ],\n       [0.58068791, 0.41931209],\n       [0.62768628, 0.37231372],\n       [0.47559883, 0.52440117],\n       [0.4267593 , 0.5732407 ],\n       [0.66172417, 0.33827583],\n       [0.55092315, 0.44907685],\n       [0.51749946, 0.48250054],\n       [0.485743  , 0.514257  ],\n       [0.49011451, 0.50988549],\n       [0.52423349, 0.47576651],\n       [0.61619519, 0.38380481],\n       [0.52696302, 0.47303698],\n       [0.63957168, 0.36042832],\n       [0.52205164, 0.47794836],\n       [0.50572852, 0.49427148],\n       [0.70706202, 0.29293798],\n       [0.55266286, 0.44733714],\n       [0.52271594, 0.47728406],\n       [0.51638863, 0.48361137],\n       [0.71331391, 0.28668609],\n       [0.67862111, 0.32137889],\n       [0.50896403, 0.49103597],\n       [0.42348082, 0.57651918],\n       [0.71495838, 0.28504162],\n       [0.59711064, 0.40288936],\n       [0.63808839, 0.36191161],\n       [0.39957895, 0.60042105],\n       [0.52127638, 0.47872362],\n       [0.65975464, 0.34024536],\n       [0.5114172 , 0.4885828 ]])\n\n\n\n\nEvaluación\nÍndice de jaccard\nProbemos el índice de Jaccard para evaluar la precisión. Podemos definir el índice de Jaccard como el tamaño de la intersección dividido por el tamaño de la unión de los dos conjuntos de etiquetas.\n\nSi el conjunto completo de etiquetas predichas para una muestra coincide exactamente con el conjunto verdadero de etiquetas, la precisión del subconjunto será 1.0.\nDe lo contrario, será 0.0.\n\n\nfrom sklearn.metrics import jaccard_score\njaccard_score(y_test, yhat,pos_label=0)\n\nnp.float64(0.7058823529411765)\n\n\nMatriz de confusión\nOtra forma de observar la presición de nuestro modelo de clasificación es crear una matriz de confusión.\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))\n\n[[ 6  9]\n [ 1 24]]\n\n\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')\n\nConfusion matrix, without normalization\n[[ 6  9]\n [ 1 24]]\n\n\n\n\n\n\n\n\n\nAnalicemos la primera fila. La primera fila corresponde a los clientes cuyo valor real de churn en el conjunto de prueba es 1.\nComo puedes calcular, de 40 clientes, el valor de churn es 1 para 15 de ellos.\nDe estos 15 casos:\n\nEl clasificador predijo correctamente 6 como 1.\nY predijo incorrectamente 9 como 0.\n\nEsto significa que:\n\nPara 6 clientes, el valor real de churn era 1 (en el conjunto de prueba) y el clasificador también los predijo correctamente como 1.\nSin embargo, para 9 clientes cuyo valor real era 1, el clasificador los predijo como 0, lo cual no es muy bueno. Podemos considerarlo como el error del modelo en la primera fila.\n\n¿Qué ocurre con los clientes con churn igual a 0? Veamos la segunda fila:\n\nHubo 25 clientes cuyo valor de churn era 0.\nEl clasificador predijo correctamente 24 de ellos como 0.\nY predijo incorrectamente 1 de ellos como 1.\n\nPor lo tanto, el modelo hizo un buen trabajo prediciendo a los clientes con churn igual a 0.\nVentaja de la matriz de confusión:\nMuestra la capacidad del modelo para predecir correctamente o separar las clases. En el caso específico de un clasificador binario (como este ejemplo), podemos interpretar estos números como:\n\nVerdaderos positivos (VP): Casos correctamente predichos como 1.\nFalsos positivos (FP): Casos predichos como 1 pero que en realidad son 0.\nVerdaderos negativos (VN): Casos correctamente predichos como 0.\nFalsos negativos (FN): Casos predichos como 0 pero que en realidad son 1.\n\n\nprint (classification_report(y_test, yhat))\n\n              precision    recall  f1-score   support\n\n           0       0.73      0.96      0.83        25\n           1       0.86      0.40      0.55        15\n\n    accuracy                           0.75        40\n   macro avg       0.79      0.68      0.69        40\nweighted avg       0.78      0.75      0.72        40\n\n\nLos resultados muestran que el modelo tiene un desempeño diferenciado entre las clases. Para la clase 0 (no churn), logra una precisión del 73%, identificando correctamente el 96% de los casos reales (recall), lo que se refleja en un sólido F1-score de 0.83. Esto indica que el modelo es particularmente bueno detectando clientes que permanecerán. Sin embargo, para la clase 1 (churn), aunque tiene una alta precisión (86%), solo captura el 40% de los casos reales, resultando en un F1-score más bajo (0.55). Esto sugiere dificultades para identificar completamente a los clientes que abandonarán. La exactitud global del modelo es del 75%, pero al analizar el promedio macro de los F1-scores (0.69) y el promedio ponderado (0.72), vemos que el rendimiento es desigual entre clases. El alto recall en clase 0 contrasta con el bajo recall en clase 1, indicando que el modelo tiende a ser conservador al predecir churn, posiblemente para minimizar falsos positivos a costa de perder algunos casos reales.\nlog loss\nAhora, probemos la pérdida logarítmica (log loss) para la evaluación. En la regresión logística, la salida puede ser la probabilidad de que el abandono del cliente sea afirmativo (o igual a 1). Esta probabilidad es un valor entre 0 y 1. La pérdida logarítmica mide el rendimiento de un clasificador donde la salida predicha es un valor de probabilidad entre 0 y 1.\n\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)\n\n0.6017092478101185\n\n\n\nLR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)\nyhat_prob2 = LR2.predict_proba(X_test)\nprint (\"LogLoss: : %.2f\" % log_loss(y_test, yhat_prob2))\n\nLogLoss: : 0.61"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Logros",
    "section": "",
    "text": "Denis Ávila, D., Curbelo, E. A., Madrigal-Roca, L. J., & Pérez-Lanyau, R. D. (2020). Spatio temporal variations of the spectral response in mangroves of Havana, Cuba, by remote sensing. Revista De Biología Tropical, 68(1), 321–335. https://doi.org/10.15517/rbt.v68i1.39134\nDenis Ávila, D., Ramírez-Arrieta, V. M., & Pérez-Lanyau, R. D. (2020). Variación espacial de la morfometría foliar en manglares de La Habana, Cuba. Rev. Biol. Trop., 68, 13. http://dx.doi.org/10.15517/rbt.v68i2.39133\nPérez-Leira, R., Pérez-Ojeda, O. L., Pérez-Ojeda, O. M., & Pérez-Lanyau, D. (2025). Estudio exploratorio sobre el régimen de riego en una parcela de pitahaya (Hylocereus undatus). Ingeniería Agrícola, 15, https://cu-id.com/2284/v15e04. Recuperado a partir de https://revistas.unah.edu.cu/index.php/IAgric/article/view/2024"
  },
  {
    "objectID": "about.html#publicaciones-científicas",
    "href": "about.html#publicaciones-científicas",
    "title": "Logros",
    "section": "",
    "text": "Denis Ávila, D., Curbelo, E. A., Madrigal-Roca, L. J., & Pérez-Lanyau, R. D. (2020). Spatio temporal variations of the spectral response in mangroves of Havana, Cuba, by remote sensing. Revista De Biología Tropical, 68(1), 321–335. https://doi.org/10.15517/rbt.v68i1.39134\nDenis Ávila, D., Ramírez-Arrieta, V. M., & Pérez-Lanyau, R. D. (2020). Variación espacial de la morfometría foliar en manglares de La Habana, Cuba. Rev. Biol. Trop., 68, 13. http://dx.doi.org/10.15517/rbt.v68i2.39133\nPérez-Leira, R., Pérez-Ojeda, O. L., Pérez-Ojeda, O. M., & Pérez-Lanyau, D. (2025). Estudio exploratorio sobre el régimen de riego en una parcela de pitahaya (Hylocereus undatus). Ingeniería Agrícola, 15, https://cu-id.com/2284/v15e04. Recuperado a partir de https://revistas.unah.edu.cu/index.php/IAgric/article/view/2024"
  },
  {
    "objectID": "about.html#premios",
    "href": "about.html#premios",
    "title": "Logros",
    "section": "Premios",
    "text": "Premios\n\nPrimer Premio, VI Congreso Estudiantil de Investigación Científica y Tecnología (CEICYT) celebrado en la Universidad del Caribe (UNICARIBE) en Santo Domingo, República Dominicana (2021) en el área temática ‘Internacional’, por el trabajo titulado: ‘Evaluación del efecto Isla de calor urbano en ciudades cubanas y sus variaciones históricas.’\nCONVOCATORIA DE SOLICITUDES: Jóvenes Investigadores Estudiantiles 2024 por el Consejo Sudcaliforniano de Ciencia y Tecnología."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sobre mi",
    "section": "",
    "text": "Master en Manejo en Ciencias de Recursos Marinos\nCientífico de datos con sólido dominio en procesamiento, visualización y SIG. En constante formación, aplico técnicas estadísticas y de machine learning, incluyendo enfoques geoespaciales, para transformar datos en decisiones estratégicas."
  },
  {
    "objectID": "index.html#habilidades",
    "href": "index.html#habilidades",
    "title": "Sobre mi",
    "section": "Habilidades",
    "text": "Habilidades\n\n\n\nHabilidades, programas, lenguajes de programación, módulos\n\n\n\nPython (pandas, numpy, scikit-learn), SQL ,R, QGIS, Google Earth Engine, Git y GitHub"
  },
  {
    "objectID": "index.html#certificaciones.",
    "href": "index.html#certificaciones.",
    "title": "Sobre mi",
    "section": "Certificaciones.",
    "text": "Certificaciones.\n\nPython\n\n“Python for Benginners” by Sololearn: [ver certificado]\n“Intermediate Python” by Sololearn: [ver certificado]\n“Python” de Kaggle: [ver certificado]\n“Pandas” de Kaggle: [ver certificado]\n“Machine Learning with Python - Level 1 by IBM”: [ver certificado]\n\n\n\nR\n\n“Theorical and Practical Understanding of R languaje” by Sololearn: [ver certificado]\n\n\n\nSQL\n\n“Intro to SQL”, de Kaggle: [ver certificado]\n“Introducción a SQL”, de Sololean: [ver certificado]\n“SQL intermedio”, de Sololearn: [ver certificado]\n\n\n\nEstadística, Ciencia de datos, Analisis de datos\n\n“Time Series Workshop” by CICIMAR-IPN: [ver certificado]\n“Data Science: Python for Data Analysis Full Bootcamp” by UDEMY: [ver certificado]\n“Data Analysis with R Programming and Python”: [ver certificado]\n“Free Tensor-Flow Keras Bootcamp” de OpenCV: [ver certificado]\n\n\n\nArcGIS ,QGIS, SNAP\n\n“Spatial Data Science: The New Frontier in Analytics” de ESRI: [ver certificado]\n“Optical Remote Sensing - Introductory Level” de “National Commission on Space Activities (CONAE)”: [ver certificado]\n“Teledetección aplicada al Color del Océano - Nivel Introductorio” de CONAE: [ver certificado]\n“Methane Observations for Large Emission Event Detection and Monitoring”: [ver certificado]\n\n\n\nGoogle Earth Engine, Python\n\n“The Complete Google Earth Engine Python API & Colab Bootcamp” de UDEMY: [ver certificado]\n\n\n\nGit, GitHub\n\n“Basics of Git, GITHUB” de Desafío Latam: [ver certificado]\n\n\n\nMatLab\n\n“MatLab Onramp” de MathWorks: [ver certificado]"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Visualización de la pluma del río Amazonas, con datos NetCDF",
    "section": "",
    "text": "En esta publicación voy a explicar como hacer una animación de corrientes oceánicas y salinidad con datos en formato NetCDF correspondientes a la desembocadura del río Amazonas con el lenguaje de programación R.\n\nActivación de librerías de R para manejo y visualización de datos geográficos.\n\n#Empieza aquí el verdadero trabajo \n\nlibrary(ncdf4)\nlibrary(raster)\nlibrary(sp)\nlibrary(sf)\nlibrary(terra)\nlibrary(RColorBrewer)\nlibrary(rasterVis)\nlibrary(spDataLarge)\nlibrary(lwgeom)\nlibrary(mapdata)\nlibrary(latticeExtra)\nlibrary(magick)\nlibrary(RColorBrewer)\nlibrary(scales)\nlibrary(lwgeom)\nlibrary(mapdata)\nlibrary(viridis)\n\n\n\nExtracción de las variables del archivo NetCDF\nEl archivo NetCDF tiene variables de temperatura, salinidad, batimetría, componente zonal, componente meridional, altimetría y densidad , entre otras. Y las variables globales latitud, longitud, tiempo y profundidad que son datos estructurados en forma de vector y que sirven para darle estructura e indexar a las anteriores variables mencionadas (temperatura, densidad, salinidad etc…) las cuales están estructuradas en formato de arreglo con 4 dimensiones : longitud, latitud, profundidad y tiempo. Este archivo NetCDF puede ser descargado aquí.\n\n#Se especifica la direccion de la base de datos NetCDF\ndirsemanal = \"A:/CICIMAR/Bases de datos/Output file of weekly climatology simulated with the ROMS model (NetCDF standard).nc\"\n#se le asigna a una variable la lectura de esos datos\nsemanal = nc_open(dirsemanal)\n#Se muestra en pantalla las variables que presenta la base de datos,\n#y sus dimensiones\nprint(semanal) #consultando metadatos del fichero\n\nFile A:/CICIMAR/Bases de datos/Output file of weekly climatology simulated with the ROMS model (NetCDF standard).nc (NC_FORMAT_CLASSIC):\n\n     9 variables (excluding dimension variables):\n        double h[lon,lat]   \n            standard_name: Bathymetry\n            long_name: Bathymetry\n            units: m\n            missing_value: NaN\n            FillValue: NaN\n        double ssh[lon,lat,time]   \n            standard_name: SSH\n            long_name: Sea Surface Height\n            units: m\n            missing_value: NaN\n            FillValue: NaN\n        double temp[lon,lat,depth,time]   \n            standard_name: temp\n            long_name: Potential temperature\n            units: degreeC\n            missing_value: NaN\n            FillValue: NaN\n        double salt[lon,lat,depth,time]   \n            standard_name: salt\n            long_name: Salinity\n            units: PSU\n            missing_value: NaN\n            FillValue: NaN\n        double swd[lon,lat,depth,time]   \n            standard_name: Density\n            long_name: Seawater density\n            units: Kg m^-3\n            missing_value: NaN\n            FillValue: NaN\n        double ucurr[lon,lat,depth,time]   \n            standard_name: ucurr\n            long_name: Zonal component\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double vcurr[lon,lat,depth,time]   \n            standard_name: vcurr\n            long_name: Meridional component\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double vort[lon,lat,depth,time]   \n            standard_name: vort\n            long_name: Vorticity\n            units: s^-1\n            missing_value: NaN\n            FillValue: NaN\n        double cspd[lon,lat,depth,time]   \n            standard_name: cspd\n            long_name: Current speed\n            units: m s^-1\n            missing_value: NaN\n            FillValue: NaN\n\n     4 dimensions:\n        lon  Size:183 \n            standard_name: logitude\n            long_name: logitude\n            units: degrees_east\n            axis: X\n        lat  Size:159 \n            standard_name: latitude\n            long_name: latitude\n            units: degrees_north\n            axis: Y\n        depth  Size:61 \n            standard_name: Depth\n            long_name: Vertical axis\n            units: m\n            axis: Z\n        time  Size:48   *** is unlimited *** \n            standard_name: time\n            long_name: time\n            calendar: standard\n            axis: T\n            comment: \n\n    5 global attributes:\n        creation_date: 05-Aug-2022 01:24:56\n        Producer: DOCEAN/UFPE/Recife/PE/Brasil\n        dataType: 2D Grid\n        Comment: ArpHDv2 datasets (standard NetCDF files)\n        Author: H. L. Varona & M. Araujo\n\n#Una vez visto cuales son las cadenas con las que se nombran a las variables globales\n#procedemos a asignarlas a variables para su posterior extracción del archivo NetCDF\nlat_variable = 'lat'\nlon_variable = 'lon'\ntime_variable ='time'\ndeepvariable= 'depth'\n\n# Extracción de las variables globales\nlatssem = ncvar_get(semanal,lat_variable) \nlonssem = ncvar_get(semanal,lon_variable)\ntimessem = ncvar_get(semanal,time_variable)\ndeepseem=ncvar_get(semanal,deepvariable)\n\n\n#En esta publicación solo vamos a usar la salinidad y los componentes zonales y meridionales para visualizar la magnitud y dirección de las corrientes de los datos\nsalisem = ncvar_get(semanal,\"salt\") \nzonal=ncvar_get(semanal,\"ucurr\")\nmeridional=ncvar_get(semanal,\"vcurr\")\n\n\n\n#Definimos un cuadrante de interes: en las longitudes desde -60 hasta los -40.5 , y en las latitudes desde las -3 hasta las 9.5\n\nlon1=-60\nlon2=-40.5\nlat1=-3\nlat2=9.5\n\n\n\n#Debemos saber que índice ocupan nuestras coordenadas de interes en las variables globales, por eso aplicamos wich para obtener dichos índices en las variables globales.\n#Estos índices serán empleados para indexar las variables ambientales (eg. salinidad) en las dimensiones 1 y 2, que corresponden  a las longitudes y latitudes respectivamente. \n\nindlon1=which(lonssem==lon1)\nindlon2=which(lonssem==lon2)\nindlat1=which(latssem==lat1)\nindlat2=which(latssem==lat2)\n\n\n#Acotando el rango de las variables de latitudes y longitudes para que pueda ser empleado correctamente con las funciones de rastervis\n\n\n#De las variables globales longitud y latitud vamos a extraer unas secciones de estas\n#correspondientes a nuestra region de interes\n#Estos vectores ranlon y ranlat serán utilizados posteriormente para georeferenciar los rasters que se elaboran a partir de las matrices o arreglos de la variable salinidad\nranlon=lonssem[indlon1:indlon2]\nranlat=latssem[indlat1:indlat2]\n\n\n\nAcote de área de interes, rasterización y georeferenciación de las capas de salinidad\nUna vez tenemos las 48 capas de tiempo de la variable de salinidad rasterizada y georeferenciada, podemos proceder a visualizar la capa numero 24 de 48. La cual podemos acceder a ella dentro del elemento 24 de la lista vacía creada para almacenar los rasteres georeferenciados de salinidad\n\n#Creamos una secuencia que se usará para asignar rangos a la variable de salinidad.\n#Desde los 24 UPS (Unidades prácticas de salinidad), hasta los 38 UPS, con saltos de 1.5 UPS\natsal&lt;- seq(24,38,1.5)\n#Le agregamos un título a la leyenda de la escala de colores\nColorkeySal &lt;- list(title=list(\"UPS\"))\n\nlevelplot(lista_vacia[[24]],at=atsal,colorkey = ColorkeySal,par.settings = viridisTheme(region = viridis(10)),margin=FALSE)+ \n  #Se añade una capa a la visualización, correspondiente al shp importado\n    layer(sp.polygons(cntry_sp_wgs84, fill = \"#b28158\"))\n\n\n\n\n\n\n\n\nFigura 1: Visualización de la salinidad en el área de interes para la capa 24.\n\n\nAcote de área de interes, rasterización y georeferenciación de las capas de componente zonal y meridional\n\n##########Creando mapa de dirección de corrientes\n\n\n#Realizamos un acote del área interes nuevamente, pero para las variables de componente zonal y componente meridional \nu=zonal[indlon1:indlon2,indlat1:indlat2,1,]\nv=meridional[indlon1:indlon2,indlat1:indlat2,1,]\n\n\n# Creamos nuevamente listas vacías para las variables de componente zonal y meridional\n\n# Creando una lista vacía con 48 elementos\nlistaU &lt;- vector(\"list\", length = 48)\n# Creando una lista vacía con 48 elementos\nlistaV &lt;- vector(\"list\", length = 48)\n\n\nfor(i in 1:48){\n  #Se rota cada elemento zonal y meridional \n  listaU[[i]] &lt;- apply(t(u[,,i]),2,rev)\n  listaV[[i]] &lt;-apply(t(v[,,i]),2,rev)\n\n  # Se convierte a raster cada elemento de la lista\n  listaU[[i]]=raster(listaU[[i]])\n  listaV[[i]]=raster(listaV[[i]])\n  #Se georeferencia cada elemento raster de las listas vacías con las coordenadas correspondientes a la región de interes\n  extent(listaU[[i]]) &lt;- extent(c(lon1,lon2,lat1,lat2))\n  extent(listaV[[i]]) &lt;- extent(c(lon1,lon2,lat1,lat2)) \n  }\n\n\n\nCreación de una animación a partir de las 48 visualizaciones de salinidad y corrientes superficiales\n\n#creamos un vecor con los nombres de los meses de un año, y que cada mes se repita 4 veces consecutivas, para asignar un mes a cada promedio semanal de las 48 capas de tiempo del archivo\nmeses=rep(c(\"enero\",\"febrero\",\"marzo\",\"abril\",\"mayo\",\"junio\",\"julio\",\"agosto\",\"septiembre\",\"octubre\",\"noviembre\",\"diciembre\"),each=4)\n\n# Inicializar una lista para almacenar cada frame del GIF\nframes &lt;- list()\n\nfor (i in 1:48) {\n  #Se introduce en un brick, al iEsimo componente zonal y meridional  que posteriormente la funcion levelplot usara para calcular la magnitud y velocidad de corrientes con ese brick\n  w &lt;- brick(listaU[[i]], listaV[[i]])\n  \n  #Al objeto w lo multiplicamos por 10 para acentuar las diferencias entre los vectores de las corrientes, como en esta gráfica el tamaño del vector no significa una velocidad específca y solo es representativo de su magnitud se acepta en este caso la multiplicación x 10.\n  u &lt;- vectorplot(w*10, isField = \"dXY\", \n                  par.settings = viridisTheme(region = viridis(10)),\n                  scaleSlope = FALSE, \n                  at = atsal, \n                  region = lista_vacia[[i]], \n                  colorkey = ColorkeySal, \n                  margin = FALSE, \n                  narrows = 1000,\n                  col.arrows = \"black\",\n                  main = paste(\"Dirección y magnitud de corrientes (m/s) y grado de salinidad, \\n mes: \",meses[i], \", semana: \", i,\"/48\", sep = \"\"),\n                  scales = list(\n                    y = list(\n                      at = c(2, 5, 8),\n                      labels = c(\"2\", \"5\", \"8\")\n                    )\n                  )) + \n    layer(sp.polygons(cntry_sp_wgs84, fill = \"#b28158\"))\n  \n  # Guardar cada plot como imagen temporal\n  png(filename = paste0(\"frame_\", sprintf(\"%02d\", i), \".png\"), width = 800, height = 600)\n  print(u)\n  dev.off()\n  \n  # Leer la imagen y añadirla a la lista de frames\n  frames[[i]] &lt;- image_read(paste0(\"frame_\", sprintf(\"%02d\", i), \".png\"))\n}\n\n# Combinar todos los frames en un GIF\ngif &lt;- image_animate(image_join(frames), fps = 5)  # 5 frames por segundo\n\n# Guardar el GIF\nimage_write(gif, \"corrientes_salinidad.gif\")\n\n\n# Opcional: Eliminar los archivos PNG temporales\nfile.remove(list.files(pattern = \"frame_.*\\\\.png\"))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE\n\n\n\n\n\nFigura 2: Secuencia de imagenes de corrientes superficiales y salinidad de la pluma del río Amazonas\n\n\n\n\nReferencias\nR Core Team (2024). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org.\nVarona, H. L., & Araujo, M. (2022). Hydro-thermodynamic dataset of the Amazon River Plume and North Brazil Current retroflection. Data in Brief, 40. https://doi.org/10.17882/82958"
  }
]